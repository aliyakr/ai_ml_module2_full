{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df86b07",
   "metadata": {},
   "source": [
    "# Модуль 2. Занятие 2. Деревья решений и ансамблевые методы в Scikit-Learn\n",
    "\n",
    "В этом ноутбуке рассмотрены деревья решений и ансамблевые методы, такие как случайный лес и бустинг. Эти методы широко используются в задачах классификации и регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9c9c0",
   "metadata": {},
   "source": [
    "## 1. Деревья решений\n",
    "\n",
    "Деревья решений — это популярный алгоритм машинного обучения, используемый для задач классификации и регрессии. Он работает по принципу разбиения данных на основе определённых признаков для предсказания целевого значения. Этот алгоритм может быть визуализирован в виде дерева, где каждый узел представляет собой признак, ветви — возможные значения этого признака, а листья — предсказания или классы.\n",
    "\n",
    "Основные понятия:\n",
    "- **Корень дерева** — начальная точка, от которой начинается разбиение данных.\n",
    "- **Узлы** — точки, где происходит проверка признака или принятие решения.\n",
    "- **Ветви** — пути, ведущие к следующему узлу или листу.\n",
    "- **Листья** — узлы, которые представляют собой конечное решение или предсказание.\n",
    "\n",
    "Алгоритм построения дерева решений основывается на выборе оптимального признака для разбиения данных на каждом уровне. Выбор признака производится с целью максимизировать 'чистоту' разбиения, используя такие метрики, как энтропия и коэффициент Джини."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d365dc",
   "metadata": {},
   "source": [
    "### Пример использования `DecisionTreeClassifier`\n",
    "\n",
    "`DecisionTreeClassifier` — модель дерева решений для классификации данных на основе признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174ea516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели дерева решений: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Загрузка данных\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "\n",
    "# Создание и обучение модели\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Точность модели дерева решений:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ed495",
   "metadata": {},
   "source": [
    "## Генерация данных для дерева решений\n",
    "\n",
    "Мы создадим набор данных с двумя классами, который будем использовать для обучения модели дерева решений. Это упрощённый двумерный набор данных, что позволяет нам легко визуализировать разделяющую поверхность, построенную деревом решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Генерация данных\n",
    "np.random.seed(7)\n",
    "\n",
    "# Первый класс данных\n",
    "train_data = np.random.normal(size=(100, 2))\n",
    "train_labels = np.zeros(100)\n",
    "\n",
    "# Второй класс данных\n",
    "train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "train_labels = np.r_[train_labels, np.ones(100)]\n",
    "\n",
    "# Визуализация данных\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, cmap=\"autumn\", edgecolors=\"black\", linewidth=1.5)\n",
    "plt.title(\"Сгенерированные данные для двух классов\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cd1a7",
   "metadata": {},
   "source": [
    "## Обучение дерева решений и визуализация разделяющей поверхности\n",
    "\n",
    "Теперь обучим модель дерева решений на сгенерированных данных и визуализируем разделяющую поверхность, построенную моделью. Мы будем использовать функцию `get_grid` для построения сетки координат, чтобы отобразить предсказания модели в виде цветной области."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Функция для создания сетки координат\n",
    "def get_grid(data, eps=0.01):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, eps), np.arange(y_min, y_max, eps))\n",
    "\n",
    "# Обучение модели дерева решений\n",
    "clf_tree = DecisionTreeClassifier(max_depth=3, criterion='gini')\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "# Построение сетки и визуализация разделяющей поверхности\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(xx, yy, predicted, cmap=\"autumn\", shading='auto')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, cmap=\"autumn\", edgecolors=\"black\", linewidth=1.5)\n",
    "plt.title(\"Разделяющая поверхность дерева решений\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227c342",
   "metadata": {},
   "source": [
    "## Визуализация структуры дерева решений\n",
    "\n",
    "Используем библиотеку `pydotplus` для визуализации структуры обученного дерева решений. Сначала экспортируем дерево в формат `.dot`, затем преобразуем его в изображение и отображаем в ноутбуке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from io import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Экспорт дерева решений в формат .dot и создание графа\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf_tree, feature_names=[\"Признак 1\", \"Признак 2\"], out_file=dot_data, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "# Отображение графа дерева решений\n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475fdfa",
   "metadata": {},
   "source": [
    "## 2. Ансамблевые методы\n",
    "\n",
    "Ансамблевые методы объединяют несколько моделей деревьев решений для повышения точности и устойчивости. В Scikit-Learn доступны несколько ансамблевых методов, таких как случайный лес, градиентный бустинг и адаптивный бустинг."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99883e",
   "metadata": {},
   "source": [
    "### Случайный лес (Random Forest)\n",
    "\n",
    "Случайный лес — ансамбль из нескольких случайных деревьев решений, обученных на различных подвыборках данных. Результат предсказания основывается на голосовании (для классификации) или усреднении (для регрессии) предсказаний всех деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Создание и обучение модели случайного леса\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Точность модели случайного леса:\", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098cc1d-b382-490a-87d2-f29c2566889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Генерация данных для случайного леса\n",
    "np.random.seed(42)\n",
    "rf_data = np.random.normal(size=(100, 2))\n",
    "rf_labels = np.zeros(100)\n",
    "\n",
    "# Добавляем второй класс\n",
    "rf_data = np.r_[rf_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "rf_labels = np.r_[rf_labels, np.ones(100)]\n",
    "\n",
    "# Обучение модели случайного леса\n",
    "rf_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_clf.fit(rf_data, rf_labels)\n",
    "\n",
    "# Построение сетки и визуализация разделяющей поверхности\n",
    "xx, yy = get_grid(rf_data)\n",
    "predicted_rf = rf_clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(xx, yy, predicted_rf, cmap=\"coolwarm\", shading='auto')\n",
    "plt.scatter(rf_data[:, 0], rf_data[:, 1], c=rf_labels, s=100, cmap=\"coolwarm\", edgecolors=\"black\", linewidth=1.5)\n",
    "plt.title(\"Разделяющая поверхность случайного леса\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cadb2a",
   "metadata": {},
   "source": [
    "### Градиентный бустинг (Gradient Boosting)\n",
    "\n",
    "Градиентный бустинг создаёт последовательность деревьев, где каждое следующее дерево корректирует ошибки предыдущих. Это мощный метод, который требует больше времени на обучение, но часто показывает лучшую точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Создание и обучение модели градиентного бустинга\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"Точность модели градиентного бустинга:\", accuracy_score(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e64b4",
   "metadata": {},
   "source": [
    "### Адаптивный бустинг (AdaBoost)\n",
    "\n",
    "Адаптивный бустинг обучает последовательность деревьев решений, где каждая следующая модель уделяет больше внимания ошибочно классифицированным примерам, чтобы улучшить точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d54354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Создание и обучение модели адаптивного бустинга\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "print(\"Точность модели адаптивного бустинга:\", accuracy_score(y_test, y_pred_ada))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0062",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "Деревья решений и ансамблевые методы представляют собой мощные инструменты для задач классификации и регрессии. Они легко интерпретируются и могут быть эффективно применены для решения многих задач. Ансамблевые методы, такие как случайный лес и бустинг, помогают повысить точность и устойчивость моделей, объединяя несколько деревьев решений в единый ансамбль."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2d208",
   "metadata": {},
   "source": [
    "## 3. Метод ближайших соседей (K-Nearest Neighbors)\n",
    "\n",
    "Метод ближайших соседей (KNN) — это простой и популярный алгоритм машинного обучения для задач классификации и регрессии. Он основан на идее, что объекты, находящиеся ближе друг к другу, имеют схожие характеристики.\n",
    "\n",
    "Для классификации объекту присваивается класс, наиболее часто встречающийся среди его ближайших соседей, а для регрессии — усреднённое значение ближайших соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df3ea4",
   "metadata": {},
   "source": [
    "### Пример использования `KNeighborsClassifier`\n",
    "\n",
    "`KNeighborsClassifier` — это модель ближайших соседей, которая используется для классификации на основе расстояний до ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5960221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Создание и обучение модели ближайших соседей\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(\"Отчет классификации для K-Nearest Neighbors:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098c121-0da0-439a-a5fb-d6a55827fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Генерация данных для метода ближайших соседей\n",
    "np.random.seed(42)\n",
    "knn_data = np.random.normal(size=(100, 2))\n",
    "knn_labels = np.zeros(100)\n",
    "\n",
    "# Добавляем второй класс\n",
    "knn_data = np.r_[knn_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "knn_labels = np.r_[knn_labels, np.ones(100)]\n",
    "\n",
    "# Обучение модели K-Nearest Neighbors\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(knn_data, knn_labels)\n",
    "\n",
    "# Построение сетки и визуализация разделяющей поверхности\n",
    "xx, yy = get_grid(knn_data)\n",
    "predicted_knn = knn_clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(xx, yy, predicted_knn, cmap=\"spring\", shading='auto')\n",
    "plt.scatter(knn_data[:, 0], knn_data[:, 1], c=knn_labels, s=100, cmap=\"spring\", edgecolors=\"black\", linewidth=1.5)\n",
    "plt.title(\"Разделяющая поверхность метода ближайших соседей (KNN)\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677393b-fa2b-42b7-a379-e6011d4b6075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
