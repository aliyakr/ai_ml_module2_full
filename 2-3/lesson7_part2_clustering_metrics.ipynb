{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модуль 2. Занятие 3. Обучение без учителя\n",
    "## Часть 2. Кластеризация. Метрики качества и метод К-средних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеризация — это задача разделения данных на группы (кластеры), так чтобы данные внутри каждой группы были максимально похожими, а данные из разных групп — максимально отличались.\n",
    "\n",
    "<br><i>Пример:</i> если у вас есть данные о клиентах, кластеризация может сгруппировать их по поведению (активность, покупки).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как оценить качество кластеризации?\n",
    "Оценка качества кластеризации сложнее, чем у классификации, потому что:\n",
    "\n",
    "- Иногда нет \"правильных меток\" (мы не знаем заранее, в каком кластере должен быть объект).\n",
    "- Нужно сравнить результат кластеризации с некоторыми критериями.\n",
    "<br><br>Есть два основных подхода:\n",
    "\n",
    "1. Внешние метрики: используют известные правильные метки данных.\n",
    "2. Внутренние метрики: оценивают качество кластеров без знаний о правильных метках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (ARI)\n",
    "\n",
    "Предполагается, что известны истинные метки объектов. Данная мера не зависит от самих значений меток, а только от разбиения выборки на кластеры. Пусть $N$ – число объектов в выборке. Обозначим через $a$ число пар объектов, имеющих одинаковые метки и находящихся в одном кластере, через $b$ – число пар объектов, имеющих различные метки и находящихся в разных кластерах. Тогда Rand Index это $$\\text{RI} = \\frac{2(a + b)}{N(N-1)}.$$ То есть это доля объектов, для которых эти разбиения (исходное и полученное в результате кластеризации) \"согласованы\". Rand Index (RI) выражает схожесть двух разных кластеризаций одной и той же выборки. Чтобы этот индекс давал значения близкие к нулю для случайных кластеризаций при любом $N$ и числе кластеров, необходимо нормировать его. Так определяется Adjusted Rand Index: $$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n",
    "\n",
    "Эта мера симметрична, не зависит от значений и перестановок меток. Таким образом, данный индекс является мерой расстояния между различными разбиениями выборки. $\\text{ARI}$ принимает значения в диапазоне $[-1, 1]$. Значения, близкие к нулю соответствуют случайным разбиениям, а положительные значения говорят о том, что два разбиения схожи (совпадают при $\\text{ARI} = 1$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Простое объяснение</b>:<br>\n",
    "Представьте, что у вас есть пара объектов (например, два клиента). ARI проверяет:\n",
    "\n",
    "- Если они в одном кластере в вашем разбиении и в истинных данных — это совпадение.\n",
    "- Если они в разных кластерах и в вашем разбиении, и в истинных данных — это тоже совпадение.<br><br>\n",
    "Интерпретация:\n",
    "\n",
    "- ARI=1: идеальное совпадение с истинными метками.\n",
    "\n",
    "- ARI=0: случайное разбиение.\n",
    "\n",
    "- ARI<0: кластеризация хуже случайного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Mutual Information (AMI)\n",
    "\n",
    "Данная мера очень похожа на $\\text{ARI}$. Она также симетрична, не зависит от значений и перестановок меток. Определяется с использованием функции [энтропии](https://en.wikipedia.org/wiki/Entropy_(information_theory), интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём). Индекс $MI$ определяется как  [взаимная информация](https://en.wikipedia.org/wiki/Mutual_information) для двух распределений, соответствующих разбиениям выборки на кластеры. Интуитивно, взаимная информация измеряет долю информации, общей для обоих разбиений: насколько информация об одном из них уменьшает неопределенность относительно другого.\n",
    "\n",
    "Аналогично $\\text{ARI}$ определяется индекс $\\text{AMI}$, позволяющий избавиться от роста индекса $MI$ с увеличением числа классов. Он принимает значения в диапазоне $[0, 1]$. Значения, близкие к нулю, говорят о независимости разбиений, а близкие к единице – об их схожести (совпадении при $\\text{AMI} = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Простое объяснение:</b>\n",
    "Если объекты в одном разбиении (кластерах) часто совпадают с объектами в другом разбиении, это говорит о схожести.\n",
    "\n",
    "Интерпретация:\n",
    "- AMI=1: идеальное совпадение.\n",
    "- AMI=0: разбиения независимы (нет совпадений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Силуэт\n",
    "\n",
    "В отличие от описанных выше метрик, данный коэффициент не предполагает знания истинных меток объектов и позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации. Сначала силуэт определяется отдельно для каждого объекта. Обозначим через $a$ – среднее расстояние от данного объекта до объектов из того же кластера, через $b$ – среднее расстояние от данного объекта до объектов из ближайшего кластера (отличного от того, в котором лежит сам объект). Тогда силуэтом данного объекта называется величина: $$s = \\frac{b - a}{\\max(a, b)}.$$\n",
    "Силуэтом выборки называется средняя величина силуэта объектов данной выборки. Таким образом, силуэт показывает, на сколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров. Данная величина лежит в диапазоне $[-1, 1]$. Значения, близкие к -1, соответствуют плохим (разрозненным) кластеризациям, значения, близкие к нулю, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения, близкие к 1, соответствуют \"плотным\", четко выделенным кластерам. Таким образом, чем больше силуэт, тем более четко выделены кластеры, и они представляют собой компактные, плотно сгруппированные облака точек.\n",
    "\n",
    "С помощью силуэта можно выбирать оптимальное число кластеров $k$ (если оно заранее не известно) – выбирается число кластеров, максимизирующее значение силуэта. В отличие от предыдущих метрик, силуэт зависит от формы кластеров и достигает больших значений на более выпуклых кластерах, получаемых с помощью алгоритмов, основанных на восстановлении плотности распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Простое объяснение:</b>\n",
    "Если объект близок к другим объектам своего кластера и далёк от объектов других кластеров, это хороший результат.\n",
    "\n",
    "Интерпретация:\n",
    "- Score>0: объект находится внутри кластера.\n",
    "- Score=0: объект находится между кластерами.\n",
    "- Score<0: объект \"неправильно\" кластеризован."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм К-средних (KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делает К-средних?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разделяет данные на группы (кластеры), чтобы объекты внутри одной группы были похожи, а объекты из разных групп отличались.\n",
    "- Каждый кластер имеет центр (называемый центроидом), вокруг которого группируются данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод k-means – это один из наиболее популярных методов кластеризации. Основная идея метода заключается в том, что на каждой итерации пересчитывается центр масс (центроид) для каждого кластера, полученного на предыдущем шаге, затем объекты снова разбиваются на кластеры согласно тому, какой из новых центроидов находится ближе.\n",
    "\n",
    "Более формально, алгоритм принимает на вход выборку $X_1, \\dots, X_N$ и параметр $k$, указывающий необходимое число кластеров. Выходом алгоритма является набор из $k$ центроидов $\\{\\mu_1, \\dots, \\mu_k\\}$, с помощью которых кластеризация осуществляется путём отнесения каждого объекту к ближайшему центроиду. Все точки внутри одного кластера ближе к центроиду этого кластера, чем к центроиду любого другого кластера.\n",
    "\n",
    "Метод может быть сформулирован как задача оптимизации, а именно, минимизации суммарного квадратичного отклонения точек кластеров от центров этих кластеров по центроидам и кластерам:\n",
    "$$\\sum_{i=1}^k \\sum_{X_n \\in C_i} ||X_n - \\mu_i||^2 \\rightarrow \\min, \\text{где $C_i$ - это $i$-ый кластер, $\\mu_i$ - это центр масс кластера $C_i$.}$$\n",
    "\n",
    "Решение такой задачи оптимизации является NP-трудной задачей, однако существует простой итеративный алгоритм, позволяющий найти локальный минимум указанного функционала. Алгоритм представляет собой последовательное чередование двух шагов до сходимости. \n",
    "\n",
    "Предположим, что как-то (например, случайно) выбраны начальные положения центроидов $\\mu_1, \\dots, \\mu_k$.\n",
    "\n",
    "1) *Этап кластеризациu.* На данном этапе происходит кластеризация выборки, как было описано выше: каждый объект относится к кластеру ближайшего к нему центроида. Формально, $$C_i = \\{X_n : ||X_n - \\mu_i|| \\leq ||X_n - \\mu_j||, \\text{ для всех $j \\in \\{1, \\dots, k\\}$}\\}.$$\n",
    "\n",
    "2) *Этап обновления центроидов.* На данном этапе центроиды пересчитываются, как центры масс только что построенных кластеров. Формально, $$\\mu_i = \\frac{1}{|C_i|}\\sum_{X_n \\in C_i} X_n.$$\n",
    "\n",
    "Этот процесс продолжается, пока центроиды и кластеризация продолжают изменяться. Алгоритм гарантированно сходится, однако не гарантируется достижение глобального минимума – а только одного из локальных минимумов. Другим недостатком алгоритма является то, что итоговая кластеризация зависит от выбора исходных центров кластеров. На практике алгоритм запускается несколько раз из различных начальных приближений, а полученные результаты некоторым образом усредняются. Стоит также отметить, что число кластеров необходимо знать заранее. Существуют различные эвристики, позволяющие выбирать в некотором смысле оптимальное число кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример: кластеризация игроков NBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: Разделить игроков NBA на группы (кластеры) по их статистике, чтобы понять, какие группы игроков имеют схожие игровые характеристики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>pos</th>\n",
       "      <th>age</th>\n",
       "      <th>bref_team_id</th>\n",
       "      <th>g</th>\n",
       "      <th>gs</th>\n",
       "      <th>mp</th>\n",
       "      <th>fg</th>\n",
       "      <th>fga</th>\n",
       "      <th>fg.</th>\n",
       "      <th>...</th>\n",
       "      <th>drb</th>\n",
       "      <th>trb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>tov</th>\n",
       "      <th>pf</th>\n",
       "      <th>pts</th>\n",
       "      <th>season</th>\n",
       "      <th>season_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quincy Acy</td>\n",
       "      <td>SF</td>\n",
       "      <td>23</td>\n",
       "      <td>TOT</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>847</td>\n",
       "      <td>66</td>\n",
       "      <td>141</td>\n",
       "      <td>0.468</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>216</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>122</td>\n",
       "      <td>171</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steven Adams</td>\n",
       "      <td>C</td>\n",
       "      <td>20</td>\n",
       "      <td>OKC</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>1197</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "      <td>0.503</td>\n",
       "      <td>...</td>\n",
       "      <td>190</td>\n",
       "      <td>332</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>71</td>\n",
       "      <td>203</td>\n",
       "      <td>265</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeff Adrien</td>\n",
       "      <td>PF</td>\n",
       "      <td>27</td>\n",
       "      <td>TOT</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>961</td>\n",
       "      <td>143</td>\n",
       "      <td>275</td>\n",
       "      <td>0.520</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>306</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>108</td>\n",
       "      <td>362</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         player pos  age bref_team_id   g  gs    mp   fg  fga    fg.  ...  \\\n",
       "0    Quincy Acy  SF   23          TOT  63   0   847   66  141  0.468  ...   \n",
       "1  Steven Adams   C   20          OKC  81  20  1197   93  185  0.503  ...   \n",
       "2   Jeff Adrien  PF   27          TOT  53  12   961  143  275  0.520  ...   \n",
       "\n",
       "   drb  trb  ast  stl  blk  tov   pf  pts     season  season_end  \n",
       "0  144  216   28   23   26   30  122  171  2013-2014        2013  \n",
       "1  190  332   43   40   57   71  203  265  2013-2014        2013  \n",
       "2  204  306   38   24   36   39  108  362  2013-2014        2013  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nba = pd.read_csv(\"./data/nba_2013.csv\")\n",
    "nba.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#делим игроков на 5 кластеров\n",
    "kmeans = KMeans(n_clusters=5, random_state=1)\n",
    "numeric_cols = nba._get_numeric_data().dropna(axis=1)\n",
    "kmeans.fit(numeric_cols)\n",
    "\n",
    "\n",
    "# Visualizing using PCA\n",
    "pca = PCA(n_components=2)\n",
    "res = pca.fit_transform(numeric_cols)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(res[:,0], res[:,1], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "plt.title('PCA')\n",
    "\n",
    "# Visualizing using 2 features: Total points vs. Total assists (очки-подачи)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(nba['pts'], nba['ast'], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Total assitances')\n",
    "\n",
    "# Visualizing using 2 features: Age vs. Minutes played (возраст-время)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(nba['age'], nba['mp'], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Minutes played');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация центроидов\n",
    "\n",
    "Метод `sklearn.KMeans` содержит параметры `n_init` (число запусков из различных начальных приближений) и `init`. Есть три способа инициализации центроидов:\n",
    "- `k-means++` – \"умная\" инициализация центроидов для ускорения сходимости.\n",
    "- `random` – случайная инициализация центроидов.\n",
    "- `ndarray` – заданная инициализация центроидов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию используется умная инициализация с 10-ю запусками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие изображений с K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одно из применений алгоритма - сжатие изображений. Этот код выполняет кластеризацию пикселей изображения с использованием метода MiniBatchKMeans для сжатия изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо обработки всех данных сразу, алгоритм работает с небольшими \"пакетами\" (mini-batches), что делает его эффективным для работы с изображениями, где данных (пикселей) много."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('./img/woman.jpg')[..., 1]\n",
    "plt.figure(figsize = (20, 12))\n",
    "plt.axis('off')\n",
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "X = img.reshape((-1, 1)) # преобразуем изображение в одномерный массив пикселей\n",
    "k_means = MiniBatchKMeans(n_clusters=3) # используем три кластера - получаем три оттенка\n",
    "k_means.fit(X) \n",
    "values = k_means.cluster_centers_ \n",
    "labels = k_means.labels_\n",
    "img_compressed = values[labels].reshape(img.shape) # все пиксели изображения заменяются на значение центроида их кластера\n",
    "plt.figure(figsize = (20, 12))\n",
    "plt.axis('off')\n",
    "plt.imshow(img_compressed, cmap = 'gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нахождение тем в текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Применим KMeans для кластеризации текстов из 4 новостных категорий.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space']\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Закодируем тексты с помощью TF-IDF признаков.** \n",
    "Оценка важности слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000,\n",
    "                             min_df=2, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**И применим к получившимся векторам метод $k$ средних.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=10)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Homogeneity: 0.490 (измеряет \"однородность\" кластера)\n",
    "- Completeness: 0.535 (измеряет насколько все объекты одного класса находятся в одном кластере)\n",
    "- V-measure: 0.511 (гармоническое среднее между Homogeneity и Completeness)\n",
    "- Adjusted Rand-Index: 0.490 (сравнивает разбиение на кластеры с истинным разбиением)\n",
    "- Silhouette Coefficient: 0.017 (измеряет насколько объекты внутри одного кластера похожи друг на друга и насколько они отличаются от объектов в других кластерах)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выведем слова, соответствующие самым весомым компонентам центров кластеров.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % (i + 1), end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация рукописных цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "adjusted_rand_score(y, kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 5)\n",
    "for ax, center in zip(axes.ravel(), kmeans.cluster_centers_):\n",
    "    ax.matshow(center.reshape(8, 8), cmap=plt.cm.gray)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "name": "lesson8_part5_clustering_metrics.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
